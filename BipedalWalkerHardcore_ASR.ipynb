{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BipedalWalkerHardcore_ASR.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jBUTMo9LRTl4","colab_type":"text"},"source":["## Install gym, tensorflow, box2d package"]},{"cell_type":"code","metadata":{"id":"ChJ_KCKURKs6","colab_type":"code","colab":{}},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\n","!pip install tensorflow==2.0.0-beta > /dev/null 2>&1\n","\n","!pip install box2d-py"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqdUnuuRUWYn","colab_type":"text"},"source":["## import package"]},{"cell_type":"code","metadata":{"id":"fYlm3ImqSGrX","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","from gym import spaces\n","from gym.utils import colorize, seeding, EzPickle\n","import Box2D\n","gymlogger.set_level(40) #error only\n","\n","import tensorflow.compat.v2 as tf\n","import random\n","import numpy as np\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","\n","import sys\n","import math\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4ThLotfUa2m","colab_type":"text"},"source":["## Training code"]},{"cell_type":"code","metadata":{"id":"SdsvsbYHSXty","colab_type":"code","colab":{}},"source":["def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env\n","\n","class Model:\n","    def __init__(self, input_size, output_size):\n","        self.weights = np.zeros((input_size, output_size))\n","        \n","    # Returns predicted action which is dot product of weights and input\n","    # if deltas are given output is dot product of input and weights updated by deltas\n","    def predict(self, inp, deltas=None):\n","        w = self.weights\n","        if deltas:\n","            w += deltas\n","        output = np.dot(inp, w)\n","        return output\n","    \n","    # returns model weights\n","    def get_weights(self):\n","        return self.weights\n","        \n","    # sets model weights\n","    def set_weights(self, weights):\n","        self.weights = weights\n","        \n","class Normalizer:\n","    def __init__(self, input_size):\n","        self.n = np.zeros( input_size)\n","        self.mean = np.zeros( input_size)\n","        self.mean_diff = np.zeros( input_size)\n","        self.std = np.zeros(input_size)\n","\n","    # given new data it updates parametest of normalizer\n","    def observe(self, x):\n","        self.n += 1.0\n","        last_mean = self.mean.copy()\n","        self.mean += (x - self.mean) / self.n\n","        self.mean_diff += (x - last_mean) * (x - self.mean)\n","        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n","\n","    # normalizes the input\n","    def normalize(self, inputs):\n","        obs_mean = self.mean\n","        obs_std = np.sqrt(self.var)\n","        return (inputs - obs_mean) / obs_std\n","\n","class Agent:\n","    GAME = 'BipedalWalkerHardcore-v2'\n","\n","    def __init__(self):\n","        self.env = wrap_env(gym.make(self.GAME))\n","        self.input_size = self.env.observation_space.shape[0]\n","        self.output_size = self.env.action_space.shape[0]\n","        self.model = Model(self.input_size, self.output_size)\n","        self.normalizer = Normalizer(self.input_size)\n","        self.noise_rate = 0.06\n","        self.alpha = 0.09\n","        self.population = 16\n","        np.random.seed(1)\n","    \n","    # plays an episode of a game\n","    def play(self, deltas=None, render=False):\n","        total_reward = 0\n","        observation = self.env.reset()\n","        n=0   \n","        while n < 2000:\n","            if render:\n","                self.env.render()\n","            self.normalizer.observe(observation)\n","            observation = self.normalizer.normalize(observation)\n","            action = self.model.predict(observation, deltas)\n","            observation, reward, done, _ = self.env.step(action)\n","            reward = max(min(reward, 1), -1)\n","            total_reward += reward\n","            n+=1\n","            if done:\n","                break\n","        return total_reward\n","        \n","    # first training method uses only one deltas each step\n","    # ( less effective )\n","    def train(self, n_steps):\n","        for step in range(n_steps):\n","            deltas = self.noise_rate * np.random.random((self.input_size, self.output_size))\n","            old_weights = self.model.get_weights()\n","            r_p = self.play(deltas=deltas,render=False)\n","            r_n = self.play(deltas,deltas,render=False)\n","            \n","            new_weights = old_weights + self.alpha * (r_p - r_n) * deltas\n","            self.model.set_weights(new_weights)\n","            if step % 500 == 0:\n","                reward = self.play(render=True)\n","                print('timesteps: ', step, 'Reward+: ',reward )\n","    \n","    def train_population(self, n_steps):\n","        for step in range(n_steps):\n","            deltas = [self.noise_rate * np.random.randn(*self.model.weights.shape) for _ in range(self.population)]\n","            positive_rewards=[0] * self.population\n","            negative_rewards=[0] * self.population\n","            old_weights = self.model.get_weights()\n","            \n","            for k, d in enumerate(deltas):\n","                self.model.set_weights(old_weights + d)\n","                positive_rewards[k] = self.play(render=False)\n","                self.model.set_weights(old_weights - d)\n","                negative_rewards[k] = self.play(render=False)\n","                \n","            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n","            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.population]\n","            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n","            \n","            update = np.zeros(self.model.weights.shape)\n","            for r_p, r_n, delta in rollouts:\n","                update += (r_p - r_n) * delta\n","            new_weights = old_weights + self.alpha * update / (self.population)\n","            self.model.set_weights(new_weights)\n","            \n","            re = False\n","            if step % 100 == 0:\n","                re = True\n","            reward = self.play(render=re)\n","            print('timesteps: ', step, 'Reward: ',reward )\n","            \n","        \n","if __name__ == '__main__':\n","    agent = Agent()\n","    agent.train_population(2000)\n","    show_video()"],"execution_count":0,"outputs":[]}]}